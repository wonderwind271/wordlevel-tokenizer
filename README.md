# wordlevel-tokenizer
An implementation of word-level tokenizer that only split based on whitespace character, unlike gpt-2 tokenizer which is subword-level
